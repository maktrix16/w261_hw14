{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We followed the footsteps of the Criteo winners, per presentation below:\n",
    "http://www.csie.ntu.edu.tw/~r01922136/kaggle-2014-criteo.pdf\n",
    "\n",
    "This is for getting only categories that appear at least 10 times, required for preprocessing B. \n",
    "\n",
    "Output is file with each line a category described in this format:\n",
    "C{index}_{category}\n",
    "i.g. C1_2S3B5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting CriteoFirstPreprocessOHE.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile CriteoFirstPreprocessOHE.py\n",
    "import sys\n",
    "import ast\n",
    "import json\n",
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "\n",
    "\n",
    "#Compile a tuple of categorical feature and its value\n",
    "def parsePointForCategoricalOnly(point):\n",
    "    \"\"\"Converts a comma separated string into a list of (featureID, value) tuples ONLY IF value is not an integer. \n",
    "    Note:\n",
    "        featureIDs should start at 0 and increase to the number of features - 1.\n",
    "\n",
    "    Args:\n",
    "        point (str): A comma separated string where the first value is the label and the rest\n",
    "            are features.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of (featureID, value) tuples.\n",
    "        [(1, \"cat\"), (1,\"dog\"), ... ]\n",
    "    \"\"\"\n",
    "    values = [x for x in point.split(\"\\t\")]\n",
    "    num_features = len(values)\n",
    "    x = []\n",
    "    \n",
    "    #get only 14th feature and after\n",
    "    index = 0\n",
    "    for i in range(14, num_features):\n",
    "            x.append((index, values[i]))\n",
    "            index +=1\n",
    "    return x\n",
    "\n",
    "#create one-hot-encoder dictionary given a raw input\n",
    "def createOneHotDict(categoricalDataOnly):\n",
    "    \"\"\"Creates a one-hot-encoder dictionary based on the input data.\n",
    "\n",
    "    Args:\n",
    "        inputData  NOW RDD of tuple of all of the features -- OLD  (RDD of lists of (int, str)): An RDD of observations where each observation is\n",
    "            made up of a list of (featureID, value) tuples.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where the keys are (featureID, value) tuples and map to values that are\n",
    "            unique integers.\n",
    "            [((3, \"cat\"),4),((1, \"dog\"),1)] \n",
    "    \"\"\"\n",
    "    #get tuples for each \n",
    "    #inputData = rawData.map(parsePointForCategoricalOnly)\n",
    "    #print parsedTrainFeat.take(1)\n",
    "    \n",
    "    #remove collectAsMap as creating dictionay would not work well with large datasets!\n",
    "    return (categoricalDataOnly\n",
    "            .flatMap(lambda x: x) #everything here and below is to create a dictionary\n",
    "            .distinct()\n",
    "            .sortByKey()\n",
    "            .zipWithIndex()      \n",
    "           )\n",
    "    #return (rawData\n",
    "    #        .map(parsePointForCategoricalOnly) #get tuple of all of the features\n",
    "    #        .flatMap(lambda x: x) #everything here and below is to create a dictionary\n",
    "    #        .distinct()\n",
    "    #        .sortByKey()\n",
    "    #        .zipWithIndex()\n",
    "    #        .collectAsMap()            \n",
    "    #       )\n",
    "\n",
    "def oneHotEncoding(rawFeats, OHEDict, numOHEFeats):\n",
    "    \"\"\"Produce a one-hot-encoding from a list of features and an OHE dictionary.\n",
    "\n",
    "    Note:\n",
    "        If a (featureID, value) tuple doesn't have a corresponding key in OHEDict it should be\n",
    "        ignored.\n",
    "\n",
    "    Args:\n",
    "        rawFeats (list of (int, str)): The features corresponding to a single observation.  Each\n",
    "            feature consists of a tuple of featureID and the feature's value. (e.g. sampleOne)\n",
    "        OHEDict (now RDD for performance reason -- OLD (dict):   A mapping of (featureID, value) to unique integer.\n",
    "        numOHEFeats (int): The total number of unique OHE features (combinations of featureID and\n",
    "            value).\n",
    "\n",
    "    Returns:\n",
    "        SparseVector: A SparseVector of length numOHEFeats with indicies equal to the unique\n",
    "            identifiers for the (featureID, value) combinations that occur in the observation and\n",
    "            with values equal to 1.0.\n",
    "    \"\"\"\n",
    "    #print >> sys.stderr, OHEDict    \n",
    "    #format of OHEDict\n",
    "    #((14, u'4a0593ee'), 0)\n",
    "    \n",
    "    featureArray = []\n",
    "    for featTuple in rawFeats:\n",
    "        \n",
    "        if featTuple in OHEDict: #tuple is in the dict \n",
    "            featureArray.append(OHEDict[featTuple])\n",
    "        else: #skip it\n",
    "            pass\n",
    "    \n",
    "    #sort the indices\n",
    "    featureArray.sort()\n",
    "    \n",
    "    #we are assuming no duplicates, thus occurrence of each feature is 1 (last parameter in SparseVector)\n",
    "    return SparseVector(numOHEFeats, featureArray,[1] * len(featureArray))\n",
    "\n",
    "#for each line of data return either sparse vector or raw data, given a dictionary\n",
    "def parseOHEPoint(point, OHEDict, getRaw=False):\n",
    "    \"\"\"Obtain the label and feature vector for this raw observation.\n",
    "\n",
    "    Note:\n",
    "        You must use the function `oneHotEncoding` in this implementation or later portions\n",
    "        of this lab may not function as expected.\n",
    "\n",
    "    Args:\n",
    "        point (str): A comma separated string where the first value is the label and the rest\n",
    "            are features.\n",
    "        OHEDict  it's now an RDD for performance reason --OLD -  (dict of (int, str) to int): Mapping of (featureID, value) to unique integer.\n",
    "        getRaw: if false, return LabelPoint with SparseVector for categorical features. \n",
    "                if true, return \"\\t\" delimited mixture of integer and categorical features \n",
    "                        withon categorical features having value of either 0 or 1 \n",
    "\n",
    "    Returns:\n",
    "        LabeledPoint: Contains the label for the observation and the one-hot-encoding of the\n",
    "            raw features based on the provided OHE dictionary.\n",
    "    \"\"\"\n",
    "    #numOHEFeats = len(OHEDict)\n",
    "    numOHEFeats = OHEDict.count()\n",
    "    #print >> sys.stderr, OHEDict\n",
    "\n",
    "    values = [x for x in point.split(\"\\t\")]\n",
    "    num_features = len(values)\n",
    "    x = []\n",
    "    \n",
    "    #per Criteo Winner's Powerpoint we will treat only 14th feature and later as categorical\n",
    "    for i in range(14, num_features):\n",
    "        x.append((i, values[i]))\n",
    "    features = oneHotEncoding(x, OHEDict, numOHEFeats)\n",
    "    \n",
    "    if getRaw: #must collect label and integer features in addition to the categorical ones\n",
    "        rawDataArray = []\n",
    "        \n",
    "        #get label and integer features\n",
    "        for i in range(0,13):\n",
    "            rawDataArray.append(values[i])\n",
    "        \n",
    "        #now convert SparseVector features to \n",
    "        rawDataArray.extend([str(int(x)) for x in features.toArray().tolist()])\n",
    "        return \"\\t\".join(rawDataArray)\n",
    "        \n",
    "    else: \n",
    "        return LabeledPoint(values[0], features)\n",
    "\n",
    "\n",
    "def main_preprocessWithOHE(input_file, limit, output_file):\n",
    "    \n",
    "    #test on sample\n",
    "    rawData = sc.textFile(input_file) #'dac_sample.txt'\n",
    "    #rawData.cache() #cache since it will be used later to generate final preprocessed output\n",
    "    \n",
    "    categoricalDataOnly = rawData.map(parsePointForCategoricalOnly).flatMap(lambda x: x) # \\get tuple of only categorical features\n",
    "    categoricalDataOnly.cache()\n",
    "    \n",
    "    #create dictionary of all of the features\n",
    "    #prelimOHEDict = categoricalDataOnly \\\n",
    "    #        .distinct() #\\\n",
    "            #.sortByKey() #\\\n",
    "            #.zipWithIndex()     \n",
    "           \n",
    "    #prelimOHEDict = createOneHotDict(rawData) \n",
    "    #print prelimOHEDict.take(1)\n",
    "    #(14, u'4a0593ee')\n",
    "   \n",
    "    #apply OHE to each data based on dictionary created above\n",
    "    #prelimOHETrainData = categoricalDataOnly.map(lambda point: parseOHEPoint(point, prelimOHEDict))\n",
    "    \n",
    "    #now get count of each features so we can prune them\n",
    "    #featCounts = (prelimOHETrainData\n",
    "    #          .flatMap(lambda lp: lp.features.indices)\n",
    "    #          .map(lambda x: (x, 1))\n",
    "    #          .reduceByKey(lambda x, y: x + y)).sortByKey()\n",
    "    featCounts = (categoricalDataOnly\n",
    "                .map(lambda x: (x,1))\n",
    "                .reduceByKey(lambda x, y: x + y)).sortByKey()\n",
    "    #print featCounts.take(30) \n",
    "    #format\n",
    "    #[((14, u'1695330e'), 2), ((14, u'169f6798'), 5), ((14, u'16a99cfb'), 12)]\n",
    "    \n",
    "    #based on the count prune and create new dictionary\n",
    "    prunedOHCDictionary = featCounts \\\n",
    "        .filter(lambda keyValue: keyValue[1] >= int(limit)) \\\n",
    "        .map(lambda x: \"C\" + str(x[0][0]) + \"_\" + str(x[0][1])) \n",
    "        \n",
    "#    print prunedOHCDictionary.take(10)\n",
    "#    return\n",
    "    \n",
    "              #.join(featCounts) #\\\n",
    "              #.filter(lambda keyValue: keyValue[1][1] >= int(limit)) \\\n",
    "              #.sortByKey() \\\n",
    "              #.map(lambda x: str(x[1][0][0]) + \"_\" + str(x[1][0][1]))  #.zipWithIndex().collectAsMap() #take(10)\n",
    "    #prunedOHCDictionary = prelimOHEDict \\\n",
    "    #          .map(lambda x: (x[0],x[1])) \\\n",
    "    #          .join(featCounts) \\\n",
    "    #          .filter(lambda keyValue: keyValue[1][1] >= int(limit)) \\\n",
    "    #          .sortByKey() \\\n",
    "    #          .map(lambda x: str(x[1][0][0]) + \"_\" + str(x[1][0][1]))  #.zipWithIndex().collectAsMap() #take(10)\n",
    "    #key, value =  prunedOHCDictionary.popitem()\n",
    "    #print key\n",
    "    #print value\n",
    "    #print prunedOHCDictionary.take(1)\n",
    "    prunedOHCDictionary.saveAsTextFile(output_file)\n",
    "\n",
    "    #parse raw data again and create features\n",
    "    #finalTrainData = rawData.map(lambda point: parseOHEPoint(point, prunedOHCDictionary, True))\n",
    "    #OHETrainDataPruned.cache()\n",
    "    #print OHETrainDataPruned.take(1)    \n",
    "    \n",
    "    #finalTrainData.saveAsTextFile(output_file)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # two arguments\n",
    "    #  1. input file\n",
    "    #  2. limit\n",
    "    #  3. output file\n",
    "    \n",
    "    if len(sys.argv) < 3:\n",
    "        print >> sys.stderr, \"Usage: input_file limit_for_frequency_of_features <output_file>\"\n",
    "        exit(-1)\n",
    "\n",
    "    [input_file, limit, output_file] = sys.argv[1:4]\n",
    "    \n",
    "    sc = SparkContext(appName=\"PreprocessOHE\")\n",
    "    main_preprocessWithOHE(input_file, limit, output_file)    \n",
    "    sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: output.text: No such file or directory\n",
      "15/12/15 18:40:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/12/15 18:40:36 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "15/12/15 18:40:36 WARN MetricsSystem: Using default name DAGScheduler for source because spark.app.id is not set.\n",
      "[Stage 0:>                                                          (0 + 2) / 2]/usr/local/spark-1.5.2-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/shuffle.py:58: UserWarning: Please install psutil to have better support with spilling\n",
      "/usr/local/spark-1.5.2-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/shuffle.py:58: UserWarning: Please install psutil to have better support with spilling\n",
      "\n",
      "real\t0m15.187s\n",
      "user\t0m13.566s\n",
      "sys\t0m1.303s\n",
      "rm: _SUCCESS: No such file or directory\n",
      "-----------------OUTPUT-----------------\n",
      "cat: output.text: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "#run locally\n",
    "\n",
    "#ensure folder wit output name does not exist\n",
    "!rm -r output.text \n",
    "\n",
    "#submit spark job (note that page_rank.py has 4 arguments)\n",
    "!time /usr/local/spark-1.5.2-bin-hadoop2.6/bin/spark-submit CriteoFirstPreprocessOHE.py dac_sample.txt 10 output\n",
    "\n",
    "#output results:\n",
    "!rm _SUCCESS\n",
    "!echo '-----------------OUTPUT-----------------'\n",
    "!cat output.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#run on aws on real dataset\n",
    "#sudo scp -i margueriteoneto2.pem ../MIDS261/Week14/CriteoFirstPreprocessOHE.py  hadoop@ec2-54-153-4-193.us-west-1.compute.amazonaws.com:.\n",
    "   \n",
    "#test on sample data\n",
    "time /usr/lib/spark/bin/spark-submit CriteoFirstPreprocessOHE.py s3://criteo-dataset/dac_sample.txt 10 s3://ucb-mids-mls-kasaneutsumi/14_4_at\n",
    "time /usr/lib/spark/bin/spark-submit CriteoFirstPreprocessOHE.py s3://criteo-dataset/rawdata/train/ 10 \\\n",
    "    s3://ucb-mids-mls-kasaneutsumi/14_4_atleast10\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
