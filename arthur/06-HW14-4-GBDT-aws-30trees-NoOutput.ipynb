{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the GBDT here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run small data set locally (2-Trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Let us write spark code here for page rank\n",
    "import os\n",
    "import sys\n",
    "\n",
    "spark_home = os.environ['SPARK_HOME'] = '/Users/maktrix/Dropbox/Berkeley/W261_ML_scale/spark-1.5.1-bin-hadoop2.6'\n",
    "# spark_home = os.environ['SPARK_HOME'] = '/usr/local/Cellar/apache-spark/1.5.0/libexec/'\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME enviroment variable is not set')\n",
    "sys.path.insert(0,os.path.join(spark_home,'python'))\n",
    "sys.path.insert(0,os.path.join(spark_home,'python/lib/py4j-0.8.2.1-src.zip'))\n",
    "execfile(os.path.join(spark_home,'python/pyspark/shell.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 1.5.0\n",
      "      /_/\n",
      "\n",
      "Using Python version 2.7.10 (default, May 28 2015 17:04:42)\n",
      "SparkContext available as sc, HiveContext available as sqlContext.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vendor:  Continuum Analytics, Inc.\n",
      "Package: mkl\n",
      "Message: trial mode expires in 28 days\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "spark_home = os.environ['SPARK_HOME'] = '/usr/local/Cellar/apache-spark/1.5.0/libexec/'\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME enviroment variable is not set')\n",
    "sys.path.insert(0,os.path.join(spark_home,'python'))\n",
    "sys.path.insert(0,os.path.join(spark_home,'python/lib/py4j-0.8.2.1-src.zip'))\n",
    "execfile(os.path.join(spark_home,'python/pyspark/shell.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 1.5.2\n",
      "      /_/\n",
      "\n",
      "Using Python version 2.7.6 (default, Sep  9 2014 15:04:36)\n",
      "SparkContext available as sc, HiveContext available as sqlContext.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "spark_home = os.environ['SPARK_HOME'] = '/usr/local/spark-1.5.2-bin-hadoop2.6/'\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME enviroment variable is not set')\n",
    "sys.path.insert(0,os.path.join(spark_home,'python'))\n",
    "sys.path.insert(0,os.path.join(spark_home,'python/lib/py4j-0.8.2.1-src.zip'))\n",
    "execfile(os.path.join(spark_home,'python/pyspark/shell.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile GBDT_Model.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.mllib.tree import GradientBoostedTrees, GradientBoostedTreesModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from types import *\n",
    "\n",
    "def parsePoint(point):\n",
    "    ##exclude the first 13 features always as they are numerical\n",
    "    return [(i,j)for i,j in enumerate(point.split(',')[14:])]\n",
    "\n",
    "def createOneHotDict(inputData):\n",
    "    sampleDistinctFeats = (inputData #.flatMap(filterNumbers)\n",
    "                         .flatMap(lambda x: x)\n",
    "                      .distinct())\n",
    "    return (sampleDistinctFeats\n",
    "                           .zipWithIndex()\n",
    "                             .collectAsMap())  \n",
    "\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "def oneHotEncoding(rawFeats, OHEDict, numOHEFeats,first13):\n",
    "    rawFeats=sorted(rawFeats)\n",
    "    num=[]\n",
    "    arr=[]\n",
    "    for j,i in enumerate(first13):\n",
    "        if i != '':\n",
    "            num.append(j)\n",
    "            arr.append(i)\n",
    "    OHEFeats=[]\n",
    "    for i in OHEDict.keys():\n",
    "        if i in rawFeats:\n",
    "            OHEFeats.append(i)\n",
    "                  \n",
    "    return SparseVector(26+13,sorted(num+\\\n",
    "                            [OHEDict[i]+13 for i in OHEFeats]),\\\n",
    "                        arr+[1.0 for i in range(len(OHEFeats))]) #[i for i in first13]+\n",
    "\n",
    "def parseOHEPoint(point, OHEDict, numOHEFeats):\n",
    "    featurelist=point.strip().split(',')\n",
    "    return LabeledPoint(featurelist[0],oneHotEncoding([(i,j) for i,j in enumerate(featurelist[14:]) ],\\\n",
    "                                                      OHEDict, 26+13,[i for i in featurelist[1:14]]))\n",
    "\n",
    "def maaro(z):\n",
    "    if z[1]=='':\n",
    "        return (-1,0)\n",
    "    else:\n",
    "        return(z,1)\n",
    "\n",
    "    \n",
    "#main GBDT function\n",
    "def run_GBDT(input_file,output_file,iterations):\n",
    "    dataRDD=sc.textFile(input_file).map(lambda x: x.replace('\\t',','))\n",
    "    #Now let us create labeled point from data\n",
    "    dataRDDParsed=dataRDD.map(parsePoint).cache()\n",
    "    featSet=dataRDDParsed.flatMap(lambda x: x).map(maaro).reduceByKey(lambda a,b: a+b).takeOrdered(26,lambda (k,v): -v)\n",
    "    #reduceByKey(lambda x,y:x+y).takeOrdered(25,lambda (k,v):-v)\n",
    "    #print featSet\n",
    "    #OHEdict=createOneHotDict(dataRDDParsed,featSet)\n",
    "    OHEdict={}\n",
    "    for i,x in enumerate(featSet):\n",
    "#         print i,x\n",
    "        OHEdict[x[0]]=i\n",
    "   \n",
    "    #print oneHotEncoding(dataRDDParsed,OHEdict,numSampleOHEFeats,)\n",
    "    #Now let us create a dictionary of points\n",
    "#     weights=[.8,.1,.1]\n",
    "#     seed=42\n",
    "#     trainRDD,validateRDD,testRDD=dataRDD.randomSplit(weights,seed)\n",
    "#     OHETrainData = trainRDD.map(lambda point: parseOHEPoint(point, OHEdict, 39))\n",
    "    OHETrainData = dataRDD.map(lambda point: parseOHEPoint(point, OHEdict, 39))\n",
    "#     print OHETrainData.take(1)\n",
    "#     print OHETrainData.count()\n",
    "\n",
    "    model = (GradientBoostedTrees.trainClassifier(OHETrainData, loss = 'logLoss', numIterations=2, \n",
    "             categoricalFeaturesInfo={}, learningRate = 0.1, maxDepth = 7, maxBins = 2))\n",
    "    \n",
    "    sc.parallelize([model.toDebugString()]).coalesce(1).saveAsTextFile(output_file)  \n",
    "    \n",
    "#Execute code    \n",
    "if __name__== '__main__':\n",
    "    if len(sys.argv) < 3:\n",
    "        print >> sys.stderr, \"Usage: page_rank <initialize sc ?> <input file> <output file> <damping factor> <number_of_iterations>\"\n",
    "        exit(-1)\n",
    "\n",
    "    input_file = sys.argv[1]\n",
    "    output_file = sys.argv[2]\n",
    "    input_iterations =int(sys.argv[3])\n",
    "    \n",
    "    sc = SparkContext(appName=\"GBDT\")\n",
    "    run_GBDT(input_file,output_file,input_iterations)\n",
    "    sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Run first Spark job to generate decision tree rules\n",
    "!rm -r gbdt_model_output_local\n",
    "!time /usr/local/Cellar/apache-spark/1.5.0/libexec/bin/spark-submit ./GBDT_Model.py dac_sample.txt gbdt_model_output_local 2\n",
    "!cat gbdt_model_output_local/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Convert_Python_Rules.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Convert_Python_Rules.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "\n",
    "def rules_python(input_file, output_file):\n",
    "    \n",
    "    spacing = '    '\n",
    "    \n",
    "    with open(input_file,'r') as f_in:\n",
    "        with open(output_file,'w+') as f_out:\n",
    "            all_lines = []\n",
    "\n",
    "            for line in f_in:\n",
    "                # print(model.toDebugString())\n",
    "    #             model.txt = model.toDebugString().split('\\n')\n",
    "\n",
    "    #             for line in model.txt:\n",
    "                text = line.split()\n",
    "                if text == []:\n",
    "                    pass\n",
    "                else:\n",
    "                    all_lines.append(text)\n",
    "\n",
    "            starting_indent = 1\n",
    "            if_indents = {0: starting_indent}\n",
    "            if_num = 0\n",
    "\n",
    "            for i in range(len(all_lines)):\n",
    "                if all_lines[i][0] == 'Tree':\n",
    "                    indents = starting_indent - 1\n",
    "                    bin_num = 0\n",
    "#                     tabs = '\\t'*starting_indent\n",
    "                    tabs = spacing*starting_indent\n",
    "                    tree_num = int(all_lines[i][1].strip(':'))\n",
    "#                     print tabs + '# Tree ' + str(tree_num)\n",
    "                    f_out.write(tabs + '# Tree ' + str(tree_num)+'\\n')\n",
    "\n",
    "                elif all_lines[i][0] == 'If':\n",
    "                    indents = indents + 1\n",
    "                    if_indents[if_num] = indents\n",
    "#                     tabs = '\\t'*indents\n",
    "                    tabs = spacing*indents\n",
    "#                     print tabs + 'if ' + all_lines[i][1] + '[' + all_lines[i][2] + ']' + all_lines[i][3] + all_lines[i][4] + ':'\n",
    "                    f_out.write(tabs + 'if ' + all_lines[i][1] + '[' + all_lines[i][2] + ']' + all_lines[i][3] + all_lines[i][4] + ':'+'\\n')\n",
    "                    if_num += 1\n",
    "\n",
    "                elif all_lines[i][0] == 'Else':\n",
    "                    indents = if_indents[if_num - 1]\n",
    "#                     tabs = '\\t'*indents\n",
    "                    tabs = spacing*indents\n",
    "#                     print tabs + 'else:'\n",
    "                    f_out.write(tabs + 'else:'+'\\n')\n",
    "                    if_num -= 1\n",
    "\n",
    "                elif all_lines[i][0] == 'Predict:':\n",
    "                    indents = indents + 1\n",
    "#                     tabs = '\\t'*indents\n",
    "                    tabs = spacing*indents\n",
    "#                     print tabs + 'bin_num[' + str(tree_num) + '] = ' + str(bin_num)\n",
    "                    f_out.write(tabs + 'bin_num[' + str(tree_num) + '] = ' + str(bin_num)+'\\n')\n",
    "                    bin_num += 1\n",
    "                    indents = indents - 1\n",
    "\n",
    "if __name__== '__main__':\n",
    "    rules_python(sys.argv[1],sys.argv[2])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run small data set on EMR (2-Trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#copy data file to S3\n",
    "!aws s3 cp dac_sample.txt s3://ucb-mids-mls-marguerite-oneto/HW14/dac_sample.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile emr_config_spark_max.json\n",
    "[\n",
    "  {\n",
    "    \"Classification\": \"spark\",\n",
    "    \"Properties\": {\n",
    "      \"maximizeResourceAllocation\": \"true\"\n",
    "    }\n",
    "  }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create cluster\n",
    "!aws emr create-cluster --name \"14_4_GBDT_test_MO\" --release-label emr-4.2.0 --applications Name=Spark --instance-count 3 --use-default-roles --ec2-attributes KeyName=margueriteoneto2 --instance-type m3.xlarge  --enable-debugging --log-uri s3://ucb-mids-mls-marguerite-oneto/HW14/14_4_GBDT --configurations file://./emr_config_spark_max.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#upload pyspark script to cluster\n",
    "!scp -i margueriteoneto2.pem GBDT_Model.py hadoop@ec2-54-153-98-6.us-west-1.compute.amazonaws.com:.\n",
    "\n",
    "#delete output folder with same name\n",
    "!aws s3 rm --recursive s3://ucb-mids-mls-marguerite-oneto/HW14/14_4_GBDT/gbdt_model_output_emr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run 1st Spark job on EMR by ssh into the cluster and submit spark job with below commands:\n",
    "- ssh -i /Users/maktrix/Dropbox/Berkeley/W261_ML_scale/w261_key2.pem hadoop@ec2-52-35-71-1.us-west-2.compute.amazonaws.com\n",
    "\n",
    "- ssh -i margueriteoneto2.pem hadoop@ec2-54-153-98-6.us-west-1.compute.amazonaws.com\n",
    "- time /usr/lib/spark/bin/spark-submit --master yarn-cluster ./GBDT_Model.py s3n://hw14/14_4_GBDT/dac_sample.txt s3n://hw14/14_4_GBDT/gbdt_model_output_emr 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#KEY STEP: insert the decision tree rules into the Python file\n",
    "#ALWAYS DO THIS STEP LOCALLY (DON'T TRY TO DO THIS IN THE CLOUD)\n",
    "!rm -r gbdt_model_output_emr\n",
    "!aws s3 cp --recursive s3://ucb-mids-mls-marguerite-oneto/HW14/14_4_GBDT/gbdt_model_output_emr gbdt_model_output_emr\n",
    "!python ./Convert_Python_Rules.py gbdt_model_output_emr/part-00000 gbdt_python_rules.txt\n",
    "!sed -i '' '/#NEW_RULES_HERE/r gbdt_python_rules.txt' GBDT2_Bins.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MO Rewrite of GDBT2_Bins.py With Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting GBDT2_Bins.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile GBDT2_Bins.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from types import *\n",
    "import hashlib\n",
    "import math\n",
    "\n",
    "#hash like 3 idiots!\n",
    "def hashLike3Idiots(strToHash,numHashBuckets):\n",
    "    return int(int(hashlib.md5(strToHash.encode('utf8')).hexdigest(), 16)%numHashBuckets)\n",
    "\n",
    "def get_hashed_bins(point, numHashBuckets,frequentCategories):\n",
    "    #input is a tuple\n",
    "    #1st item is row label,2nd is sparse vector, 3rd item is a tuple (category index array, category value array)\n",
    "    #[(u'0', \n",
    "    #(SparseVector(39, {0: 1.0, 1: 1.0, 2: 5.0, 3: 0.0, 4: 1382.0, 5: 4.0, 6: 15.0, 7: 2.0, 8: 181.0, 9: 1.0, 10: 2.0, 12: 2.0, 13: 1.0, 14: 1.0, 17: 1.0, 19: 1.0, 22: 1.0, 25: 1.0, 28: 1.0, 29: 1.0, 31: 1.0, 32: 1.0}), \n",
    "    #([39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 61, 62, 63, 64], [u'68fd1e64', u'80e26c9b', u'fb936136', u'7b4723c4', u'25c83c98', u'7e0ccccf', u'de7995b8', u'1f89b562', u'a73ee510', u'a8cd5504', u'b2cb9c98', u'37c9c164', u'2824a5f6', u'1adce6ef', u'8ba8b39a', u'891b62e7', u'e5ba7672', u'f54016b9', u'21ddcdc9', u'b1252a9d', u'07b5194c', u'3a171ecb', u'c5c50484', u'e8b83407', u'9727dd16'])))]\n",
    "    \n",
    "    feature = SparseVector.toArray(point[1][0])\n",
    "    \n",
    "    bin_num = {}\n",
    "    \n",
    "#NEW_RULES_HERE\n",
    "    \n",
    "    final_features = []\n",
    "    for i in range(30):\n",
    "        string = str(i+1) + \":\" + str(int(bin_num[i]))\n",
    "        hashed_bin = int(int(hashlib.md5(string.encode('utf8')).hexdigest(), 16)%numHashBuckets)\n",
    "        final_features.append(hashed_bin)\n",
    "    \n",
    "    #now handle integer, #first 13 items in feature vector is an integer\n",
    "    for i in range(14):\n",
    "        #create text like \"I1-3\"\n",
    "        key = \"I\" + str(i)\n",
    "        featureValue = feature[i]\n",
    "        #if len(featureValue) == 0: #empty value will be treated as zero\n",
    "        #    featureValue = 0\n",
    "        \n",
    "        if int(featureValue) > 2:\n",
    "            featureValue = int(math.log(float(featureValue))**2)\n",
    "        else: \n",
    "            featureValue = 'SP'+str(featureValue)\n",
    "        final_features.append(hashLike3Idiots(key+\"-\" + str(featureValue),numHashBuckets))\n",
    "        #final_features.append(key+\"-\" + str(featureValue))\n",
    "\n",
    "    #now handle cateogry\n",
    "    for index, featureValue in enumerate(point[1][1][1]):\n",
    "        key = \"C\" + str(index)\n",
    "\n",
    "        #create text like \"C16_913ff151\"\n",
    "        keyValueString = key + \"_\" + featureValue\n",
    "\n",
    "        #print keyValueString\n",
    "        if not keyValueString in frequentCategories: #did this cat happen more than 10 times? \n",
    "            keyValueString = key + 'less' #per 3 idiots' source code\n",
    "\n",
    "        #final_features.append(keyValueString)\n",
    "        final_features.append(hashLike3Idiots(keyValueString,numHashBuckets))\n",
    "\n",
    "    label = point[0]\n",
    "\n",
    "    #print final_features\n",
    "    \n",
    "#    new_point = LabeledPoint(point.label, final_features)\n",
    "    return getFinalLabeledPoint(label, final_features,numHashBuckets)\n",
    "\n",
    "def parsePoint(point):\n",
    "    ##exclude the first 13 features always as they are numerical\n",
    "    return ([(i,j)for i,j in enumerate(point.split(',')[14:])])\n",
    "\n",
    "def createOneHotDict(inputData):\n",
    "    sampleDistinctFeats = (inputData #.flatMap(filterNumbers)\n",
    "                         .flatMap(lambda x: x)\n",
    "                      .distinct())\n",
    "    return (sampleDistinctFeats\n",
    "                           .zipWithIndex()\n",
    "                             .collectAsMap())  \n",
    "\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "def oneHotEncoding(rawFeats, OHEDict, numOHEFeats,first13, last26):\n",
    "    \n",
    "    rawFeats=sorted(rawFeats)\n",
    "    num13=[]\n",
    "    arr13=[]\n",
    "    for j,i in enumerate(first13):\n",
    "        if i != '':\n",
    "            num13.append(j)\n",
    "            arr13.append(i)\n",
    "    num26=[]\n",
    "    arr26=[]\n",
    "    for j,i in enumerate(last26):\n",
    "        if i != '':\n",
    "            num26.append(j + 39)\n",
    "            arr26.append(i)\n",
    "    OHEFeats=[]\n",
    "    for i in rawFeats:\n",
    "        if i in OHEDict.keys():\n",
    "            OHEFeats.append(i)\n",
    "\n",
    "    return (SparseVector(13+26, \\\n",
    "                        sorted(num13+[OHEDict[i]+13 for i in OHEFeats]),\\\n",
    "                        arr13+[1.0 for i in range(len(OHEFeats))]),\n",
    "                        (num26,arr26))\n",
    "            \n",
    "#    return SparseVector(13+26+26,sorted(num13+\\\n",
    "#                            [OHEDict[i]+13 for i in OHEFeats]+num26),\\\n",
    "#                        arr13+[1.0 for i in range(len(OHEFeats))]+arr26)\n",
    "\n",
    "\n",
    "def parseOHEPoint(point, OHEDict, numOHEFeats):\n",
    "    #input\n",
    "    #(u'0,1,1,5,0,1382,9727dd16, so on... ')\n",
    "    featurelist=point.strip().split(',')\n",
    "\n",
    "    new_point = (featurelist[0],oneHotEncoding([(i,j) for i,j in enumerate(featurelist[14:]) ],\\\n",
    "                                                      OHEDict, 13+26+26,[i for i in featurelist[1:14]], \\\n",
    "                                                                        [i for i in featurelist[14:]]))\n",
    "\n",
    "    return new_point\n",
    "\n",
    "def maaro(z):\n",
    "    if z[1]=='':\n",
    "        return (-1,0)\n",
    "    else:\n",
    "        return(z,1)\n",
    "\n",
    "\"\"\"\n",
    "def parseNonGBDTFeatures(point, frequentCategories,numHashBuckets):\n",
    "    #format\n",
    "    #(u'0,1,1,5,0,1382,9727dd16, so on... ', 0) \n",
    "\n",
    "    indexCount = 0\n",
    "    catCount = 0\n",
    "    features = point[0].split(',')\n",
    "\n",
    "    modFeatures = []\n",
    "    \n",
    "    #print features\n",
    "    \n",
    "    for index, featureValue in enumerate(features):\n",
    "        if index != 0: #ignore first one (label)\n",
    "            if index <14: #integer\n",
    "                #create text like \"I1-3\"\n",
    "                key = \"I\" + str(index)\n",
    "                #print featureValue\n",
    "                #print index\n",
    "                if len(featureValue) == 0: #empty value will be treated as zero\n",
    "                    featureValue = 0\n",
    "                    \n",
    "                if int(featureValue) > 2:\n",
    "                    featureValue = int(math.log(float(featureValue))**2)\n",
    "                else: \n",
    "                    featureValue = 'SP'+str(featureValue)\n",
    "                modFeatures.append(hashLike3Idiots(key+\"-\" + str(featureValue),numHashBuckets))\n",
    "                indexCount +=1\n",
    "            else:\n",
    "\n",
    "                key = \"C\" + str(catCount)\n",
    "                \n",
    "                #create text like \"C16_913ff151\"\n",
    "                keyValueString = key + \"_\" + featureValue\n",
    "                \n",
    "                #print keyValueString\n",
    "                \n",
    "                if not keyValueString in frequentCategories: #did this cat happen more than 10 times? \n",
    "                    keyValueString = key + 'less' #per 3 idiots' source code\n",
    "                    \n",
    "                modFeatures.append(hashLike3Idiots(keyValueString,numHashBuckets))\n",
    "                \n",
    "                catCount +=1\n",
    "    \n",
    "    #print indexCount\n",
    "    #print catCount\n",
    "    #output format (rowindex, label, features)\n",
    "    \n",
    "    #print modFeatures\n",
    "    label = int(features[0])\n",
    "    rowIndex = point[1]\n",
    "    return (rowIndex,(label,modFeatures))\n",
    "\"\"\"\n",
    "\n",
    "#get Labeled Point based on my arr\n",
    "def getFinalLabeledPoint(label, myarr,numBuckets):\n",
    "    \n",
    "    #remove duplicates\n",
    "    myarr = sorted(list(set(myarr)))\n",
    "\n",
    "    keyArray = []\n",
    "    countArray = []\n",
    "\n",
    "    for item in myarr:\n",
    "        keyArray.append(item)\n",
    "        countArray.append(1)\n",
    "    #print numBuckets\n",
    "    #print keyArray\n",
    "    #print countArray\n",
    "    #return SparseVector(numBuckets,keyArray,countArray)\n",
    "    return LabeledPoint(label,SparseVector(numBuckets,keyArray,countArray))\n",
    "\n",
    "def processFinalJoin(point,numBuckets):\n",
    "    \n",
    "    #(x[1][0][0],x[1][0][1] + x[1][1][1]))\n",
    "    label = point[1][0][0]\n",
    "    features = point[1][0][1] + point[1][1][1]  \n",
    "    return getFinalLabeledPoint(label, features,numBuckets)\n",
    "\n",
    "def run_bins(input_file,output_file, numHashBuckets, frequentCategories_file):\n",
    "    #repeat of before to generate OHETrainDataBins\n",
    "    dataRDD=sc.textFile(input_file).map(lambda x: x.replace('\\t',','))  #.zipWithIndex()\n",
    "    #format\n",
    "    #last integer is an index of the record, we will use it to join result of GBDT with additional Preprocessing B result\n",
    "    #(u'0,1,1,5,0,1382,9727dd16, so on... ', 0) \n",
    "    dataRDD.cache()\n",
    "\n",
    "#############################    \n",
    "##### Begin Preprocessing B for integer and categories\n",
    "#     frequentCategoriesRDD = sc.textFile(frequentCategories_file).collect() #format-- array of text like C14_23425\n",
    "    \n",
    "#     freqCategories = sc.broadcast(frequentCategoriesRDD)\n",
    "#     modifiedIntAndCat = dataRDD.map(lambda x: parseNonGBDTFeatures(x,freqCategories.value, numHashBuckets))\n",
    "#     #print modifiedIntAndCat.take(1)\n",
    "#     #format (HASHED of below)\n",
    "#     #[(rowIndex, (label, ['I1-SP1', 'I2-SP1', 'I3-2', 'I4-SP0', 'I5-52', 'I6-1', 'I7-7', 'I8-SP2', 'I9-27', 'I10-SP1', 'I11-SP2', 'I12-SP0', 'I13-SP2', u'C0_68fd1e64', u'C1_80e26c9b', 'C2less', 'C3less', u'C4_25c83c98', u'C5_7e0ccccf', u'C6_de7995b8', u'C7_1f89b562', u'C8_a73ee510', u'C9_a8cd5504', u'C10_b2cb9c98', 'C11less', u'C12_2824a5f6', u'C13_1adce6ef', u'C14_8ba8b39a', 'C15less', u'C16_e5ba7672', u'C17_f54016b9', u'C18_21ddcdc9', u'C19_b1252a9d', 'C20less', u'C21_', u'C22_3a171ecb', u'C23_c5c50484', u'C24_e8b83407', 'C25less']))]  \n",
    "##### END Preprocessing B for integer and categories\n",
    "#############################    \n",
    "\n",
    "    frequentCategoriesRDD = sc.textFile(frequentCategories_file).collect() #format-- array of text like C14_23425\n",
    "    freqCategories = sc.broadcast(frequentCategoriesRDD)\n",
    "    \n",
    "############################    \n",
    "##### Begin GBDT processing\n",
    "\n",
    "    dataRDDParsed=dataRDD.map(parsePoint).cache()\n",
    "    #format\n",
    "    #([....(24, u'e8b83407'), (25, u'9727dd16')], 0)\n",
    "    \n",
    "    #print dataRDDParsed.take(1)\n",
    "    featSet=dataRDDParsed.flatMap(lambda x: x).map(maaro) \\\n",
    "        .reduceByKey(lambda a,b: a+b).takeOrdered(26,lambda (k,v): -v)\n",
    "        \n",
    "    OHEdict={}\n",
    "    for i,x in enumerate(featSet):\n",
    "        OHEdict[x[0]]=i\n",
    "\n",
    "    OHETrainData = dataRDD.map(lambda point: parseOHEPoint(point, OHEdict, 13+26+26))\n",
    "    \n",
    "    #print OHETrainData.take(1)\n",
    "    #return \n",
    "    #OHETrainData should now be in the format:\n",
    "    #(SparseVector(65, [0, ..., 64], [I0, I1, ..., I12, OHE0, OHE1, ..., OHE25, C0, C1, ..., C25]))\n",
    "    #print \"OHETrain\"\n",
    "    #print OHETrainData.take(1)\n",
    "        \n",
    "    #apply the new decision tree\n",
    "#     OHETrainDataDense = OHETrainData.map(lambda x: SparseVector.toArray(x.features))\n",
    "#     OHETrainDataBins = OHETrainDataDense.map(lambda x: get_bins(x))\n",
    "#     OHETrainDataBins.coalesce(1).saveAsTextFile(output_file)\n",
    "\n",
    "\n",
    "    OHETrainDataBins = OHETrainData.map(lambda point: get_hashed_bins(point, numHashBuckets,freqCategories.value))\n",
    "\n",
    "\n",
    "#   OHETrainDataBins.saveAsTextFile(output_file)   \n",
    "    #OHETrainDataBins format:\n",
    "    #(rowindex, (label, [hashed GBDT assignment 232, 384, 576, 952, 528, 960, 984, 600, 552, 640, 576, 296, 144, 344, 744, 480, 832, 96, 536, 256, 304, 264, 160, 632, 960, 312, 440, 400, 264, 304]))]\n",
    "##### End GBDT processing    \n",
    "############################    \n",
    "    \n",
    "    \n",
    "############Now merge set of two features\n",
    "    #joined = OHETrainDataBins.join(modifiedIntAndCat)\n",
    "    #after join\n",
    "    #[(0, ((u'0', [232, 384, 576, 952, 528, 960, 984, 600, 552, 640, 576, 296, 144, 344, 744, 480, 832, 96, 536, 256, 304, 264, 160, 632, 960, 312, 440, 400, 264, 304]),\n",
    "     #     (0, [480, 720, 688, 856, 392, 24, 184, 480, 432, 696, 16, 368, 352, 368, 368, 160, 608, 392, 680, 768, 936, 432, 424, 112, 840, 264, 536, 152, 80, 440, 360, 640, 600, 520, 304, 128, 592, 952, 536])))]\n",
    "\n",
    "    #joinedProcessed = joined.map(lambda x: (x[1][0][0],x[1][0][1] + x[1][1][1]))\n",
    "    #joinedProcessed.cache()\n",
    "    #print joinedProcessed.take(1)\n",
    "\n",
    "    #print joinedProcessed.map(lambda x: getFinalLabeledPoint(x[0], x[1], numHashBuckets)).take(1)\n",
    "\n",
    "    #return \n",
    "#     FinalLabeledPoint =  OHETrainDataBins.join(modifiedIntAndCat).map(lambda x: processFinalJoin(x,numHashBuckets))\n",
    "\n",
    "    #freqCategories.unpersist()\n",
    "    MLUtils.saveAsLibSVMFile(OHETrainDataBins, output_file) \n",
    "    \n",
    "\n",
    "if __name__== '__main__':\n",
    "    \n",
    "    if len(sys.argv) < 4:\n",
    "        print >> sys.stderr, \"Usage: <input file> <output file> <number of hash buckets>  <frequentCategories>\"\n",
    "        exit(-1)\n",
    "\n",
    "    input_file = sys.argv[1]\n",
    "    output_file = sys.argv[2]\n",
    "    numHashBuckets = float(sys.argv[3])\n",
    "    freqCats = sys.argv[4]\n",
    "    \n",
    "    sc = SparkContext(appName=\"GBDT2\")\n",
    "    run_bins(input_file,output_file, numHashBuckets,freqCats)\n",
    "    sc.stop()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rerun Merge of Tree into New GBDT2_Bins.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://ucb-mids-mls-kasaneutsumi/14_4/_SUCCESS to gbdt_model_output_emr/_SUCCESS\n",
      "download: s3://ucb-mids-mls-kasaneutsumi/14_4/part-00000 to gbdt_model_output_emr/part-00000\n"
     ]
    }
   ],
   "source": [
    "#KEY STEP: insert the decision tree rules into the Python file\n",
    "#ALWAYS DO THIS STEP LOCALLY (DON'T TRY TO DO THIS IN THE CLOUD)\n",
    "!rm -r gbdt_model_output_emr\n",
    "!aws s3 cp --recursive s3://ucb-mids-mls-kasaneutsumi/14_4 gbdt_model_output_emr\n",
    "!python ./Convert_Python_Rules.py gbdt_model_output_emr/part-00000 gbdt_python_rules.txt\n",
    "!sed -i '' '/#NEW_RULES_HERE/r gbdt_python_rules.txt' GBDT2_Bins.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[232, 384, 576, 952, 528, 960, 984, 600, 552, 640, 576, 296, 144, 344, 744, 480, 832, 96, 536, 256, 304, 264, 160, 632, 960, 312, 440, 400, 264, 304, 480, 720, 688, 856, 392, 24, 184, 480, 432, 696, 16, 368, 352, 368, 368, 160, 608, 392, 680, 768, 936, 432, 424, 112, 840, 264, 536, 152, 80, 440, 360, 640, 600, 520, 304, 128, 592, 952, 536]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "bleh = [(0, ((u'0', [232, 384, 576, 952, 528, 960, 984, 600, 552, 640, 576, 296, 144, 344, 744, 480, 832, 96, 536, 256, 304, 264, 160, 632, 960, 312, 440, 400, 264, 304]),\n",
    "         (0, [480, 720, 688, 856, 392, 24, 184, 480, 432, 696, 16, 368, 352, 368, 368, 160, 608, 392, 680, 768, 936, 432, 424, 112, 840, 264, 536, 152, 80, 440, 360, 640, 600, 520, 304, 128, 592, 952, 536])))]\n",
    "\n",
    "print bleh[0][1][0][0]\n",
    "print bleh[0][1][0][1] + bleh[0][1][1][1]\n",
    "\n",
    "\n",
    "for i in range(14):\n",
    "    print i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run New GBDT2_Bins.py Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: gbdt_bins_output_local: No such file or directory\n",
      "15/12/16 02:08:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/12/16 02:08:47 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "15/12/16 02:08:47 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "15/12/16 02:08:47 WARN MetricsSystem: Using default name DAGScheduler for source because spark.app.id is not set.\n",
      "[Stage 1:>                                                          (0 + 2) / 2]/usr/local/spark-1.5.2-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/shuffle.py:58: UserWarning: Please install psutil to have better support with spilling\n",
      "/usr/local/spark-1.5.2-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/shuffle.py:58: UserWarning: Please install psutil to have better support with spilling\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kasane/MIDS261/Week14/./GBDT2_Bins.py\", line 11794, in <module>\n",
      "    run_bins(input_file,output_file, numHashBuckets,freqCats)\n",
      "  File \"/Users/kasane/MIDS261/Week14/./GBDT2_Bins.py\", line 11779, in run_bins\n",
      "    MLUtils.saveAsLibSVMFile(OHETrainDataBins, output_file) \n",
      "  File \"/usr/local/spark-1.5.2-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/mllib/util.py\", line 147, in saveAsLibSVMFile\n",
      "  File \"/usr/local/spark-1.5.2-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/rdd.py\", line 1508, in saveAsTextFile\n",
      "  File \"/usr/local/spark-1.5.2-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 538, in __call__\n",
      "  File \"/usr/local/spark-1.5.2-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\", line 300, in get_return_value\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o67.saveAsTextFile.\n",
      ": org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/Users/kasane/MIDS261/Week14/gbdt_bins_output_local_betterPerf already exists\n",
      "\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:132)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1089)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1065)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1065)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:310)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1065)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:989)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:965)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:965)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:310)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:965)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:897)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:897)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:897)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:310)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:896)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1430)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1409)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1409)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:310)\n",
      "\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1409)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:522)\n",
      "\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:47)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:497)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:259)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "\n",
      "\n",
      "real\t0m12.390s\n",
      "user\t0m13.265s\n",
      "sys\t0m1.320s\n",
      "cat: gbdt_bins_output_local/part-00000: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "#Run second Spark job locally to place data into bins\n",
    "!rm -r gbdt_bins_output_local\n",
    "!time /usr/local/spark-1.5.2-bin-hadoop2.6/bin/spark-submit \\\n",
    "     ./GBDT2_Bins.py dac_sample.txt gbdt_bins_output_local_betterPerf 1e3 output/    \n",
    "#!time /usr/local/Cellar/apache-spark/1.5.0/libexec/bin/spark-submit \\\n",
    "#    ./GBDT2_Bins.py dac_sample.txt gbdt_bins_output_local 1e3 output/        \n",
    "!cat gbdt_bins_output_local/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NEW NEW NEW EMR\n",
    "\n",
    "\n",
    "nohup time /usr/lib/spark/bin/spark-submit --conf spark.python.worker.reuse=false ./GBDT2_Bins.py s3://criteo-dataset/rawdata/train/ s3://ucb-mids-mls-kasaneutsumi/14_4_BeforeFFM_train 1e6 s3://ucb-mids-mls-kasaneutsumi/14_4_atleast10/ &"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload New GBDT2_Bins.py to Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import SparseVector\n",
    "aDense = np.array([0., 3., 0., 4.])\n",
    "aSparse = SparseVector(len(aDense), [1,3],[3,4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#upload second pyspark script to cluster\n",
    "!scp -i margueriteoneto2.pem GBDT2_Bins.py hadoop@ec2-54-153-4-193.us-west-1.compute.amazonaws.com:.\n",
    "\n",
    "#delete output folder with same name\n",
    "!aws s3 rm --recursive s3://ucb-mids-mls-marguerite-oneto/HW14/gbdt_bins_output_emr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run 2nd Spark job on EMR by ssh into the cluster and submit spark job with below commands:\n",
    "For Arthur:\n",
    "- ssh -i /Users/maktrix/Dropbox/Berkeley/W261_ML_scale/w261_key2.pem hadoop@ec2-52-35-71-1.us-west-2.compute.amazonaws.com\n",
    "- time /usr/lib/spark/bin/spark-submit --master yarn-cluster ./GBDT2_Bins.py s3n://hw14/14_4_GBDT/dac_sample.txt s3n://hw14/14_4_GBDT/gbdt_bins_output_emr\n",
    "\n",
    "For Marguerite:\n",
    "- ssh -i margueriteoneto2.pem hadoop@ec2-54-153-4-193.us-west-1.compute.amazonaws.com\n",
    "- time /usr/lib/spark/bin/spark-submit --master yarn-cluster ./GBDT2_Bins.py s3://ucb-mids-mls-marguerite-oneto/HW14/dac_sample.txt s3://ucb-mids-mls-marguerite-oneto/HW14/gbdt_bins_output_emr 1e3\n",
    "\n",
    "- real\t1m7.903s\n",
    "user\t0m7.752s\n",
    "sys\t0m0.904s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get the bins results\n",
    "!rm -r gbdt_bins_output_emr\n",
    "!aws s3 cp --recursive s3://ucb-mids-mls-marguerite-oneto/HW14/14_4_GBDT/gbdt_bins_output_emr gbdt_bins_output_emr\n",
    "!cat gbdt_bins_output_emr/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run 2nd Spark job on EMR by ssh into the cluster and submit spark job with below commands using Criteo training data:\n",
    "For Marguerite:\n",
    "- ssh -i margueriteoneto2.pem hadoop@ec2-54-153-4-193.us-west-1.compute.amazonaws.com\n",
    "- time /usr/lib/spark/bin/spark-submit --master yarn-cluster ./GBDT2_Bins.py s3://criteo-dataset/rawdata/train/ s3://ucb-mids-mls-marguerite-oneto/HW14/gbdt_bins_output_criteo 1e3\n",
    "\n",
    "- real\t0m55.073s  \n",
    "user\t0m7.396s  \n",
    "sys\t0m0.984s  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

