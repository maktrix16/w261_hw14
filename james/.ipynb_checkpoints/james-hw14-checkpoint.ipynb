{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW14.3 Field-aware Factorization Machine  FFM test\n",
    "**Download the Spark libFM from https://github.com/zhengruifeng/spark-libFM <br><br>\n",
    "Run the following Field-aware Factorization Machine test: <br><br>\n",
    "https://github.com/zhengruifeng/spark-libFM/blob/master/src/main/scala/TestFM.scala <br><br>\n",
    "Describe the dataset. Describe the two experiments: fm1 and fm2 and discuss your results.**\n",
    "\n",
    "**Dataset:**<br>\n",
    "The dataset comprises approximately 2.4M lines of the format: <br>\n",
    "*label feature0:value0 feature1:value1 ... featureN:valueN*\n",
    "\n",
    "The lines are in sparse format, the labels take values -1 or 1, and the feature values are either binary or take a fractional value between 0 and 1. The total uncompressed data size is about 2.2G.\n",
    "\n",
    "**Differences in the two factorizations:**\n",
    "* fm1 trains an FFM using stochastic gradient descent for 100 iterations. This is an iterative method that processes one data point at a time and adjusts the model based on the gradient of the loss function. fm1 uses all of the data in each iteration (it can be configured to use lesS).\n",
    "* fm2 trains an FFM using limited-memory Broyden–Fletcher–Goldfarb–Shanno for 20 iterations. This algorithm approximates Newton's method (using the gradient and an approximate Hessian) to find a more direct path to convergence than gradient descent.\n",
    "\n",
    "Both experiments use equivalent parameters otherwise, including for interactions (use global bias term, use one-way interactions, use 4 factors for pairwise interactions) and regularization (all regularization parameters set to 0). There is no output from FFMTest, but we can compare runtimes: with 10 m3.xlarge instances, fm1 required approximately 2 minutes and fm2 13 minutes (15 minutes total).\n",
    "\n",
    "The original scala code provides no output, so I wrote a function and created an additional RDD calculate log loss. Both fm1 and fm2 had identical log loss of 0.8520897668623415.\n",
    "\n",
    "Below are the commands and code used to set up the cluster and compile and run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up a cluster of 10 m3.xlarge instances\n",
    "./spark/ec2/spark-ec2 --key-pair=jamesr261 --identity-file=jamesr261.pem -s 12 -t m3.xlarge \\\n",
    "    --region=us-west-1 --zone=us-west-1a launch jr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the scala file for running the factorizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import com.github.fommil.netlib.BLAS._\n",
    "import com.github.fommil.netlib.BLAS.{getInstance => blas}\n",
    "\n",
    "import scala.util.Random\n",
    "\n",
    "import org.apache.spark.mllib.linalg._\n",
    "import org.apache.spark.mllib.evaluation.RegressionMetrics\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.apache.spark.mllib.regression._\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "\n",
    "object TestFM extends App {\n",
    "\n",
    "  override def main(args: Array[String]): Unit = {\n",
    "\n",
    "    val sc = new SparkContext(new SparkConf().setAppName(\"TESTFM\"))\n",
    "\n",
    "    val training = MLUtils.loadLibSVMFile(sc, \"s3n://<access_key>:<secret_key>@ucb-mids-mls-jamesroute-hw5/url_combined\", false, -1, 20).cache()\n",
    "\n",
    "    //    val task = args(1).toInt\n",
    "    //    val numIterations = args(2).toInt\n",
    "    //    val stepSize = args(3).toDouble\n",
    "    //    val miniBatchFraction = args(4).toDouble\n",
    "\n",
    "    val dataSize = training.count()\n",
    "\n",
    "    // run with SGD, report log loss\n",
    "    val fm1 = FMWithSGD.train(training, task = 1, numIterations = 100, stepSize = 0.15, miniBatchFraction = 1.0, dim = (true, true, 4), regParam = (0, 0, 0), initStd = 0.1)\n",
    "    val logLoss_fm1 = training.map { point =>\n",
    "      fm1.predict(point.features)\n",
    "    }.sum() / dataSize\n",
    "    println(\"training log loss for fm1 = \" + logLoss_fm1)\n",
    "\n",
    "    // run with LBFGS, report log loss \n",
    "    val fm2 = FMWithLBFGS.train(training, task = 1, numIterations = 20, numCorrections = 5, dim = (true, true, 4), regParam = (0, 0, 0), initStd = 0.1)\n",
    "    val logLoss_fm2 = training.map { point =>\n",
    "      fm2.predict(point.features)\n",
    "    }.sum() / dataSize\n",
    "    println(\"training log loss for fm2 = \" + logLoss_fm2)\n",
    "\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the SBT file used to build the jar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name := \"TESTFM\"\n",
    "\n",
    "version := \"1.0\"\n",
    "\n",
    "scalaVersion := \"2.10.4\"\n",
    "\n",
    "libraryDependencies ++= Seq(\n",
    "  \"org.apache.spark\" % \"spark-core_2.10\" % \"1.5.1\",\n",
    "  \"org.apache.spark\" % \"spark-mllib_2.10\" % \"1.5.1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the command to submit the job to Spark, run from the root of the project directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# driver mem set to 6G to avoid java heap memory issues\n",
    "~/spark/bin/spark-submit --driver-memory 6G \\\n",
    "    --master spark://ec2-52-53-250-84.us-west-1.compute.amazonaws.com:7077 \\\n",
    "    target/scala-2.10/testfm_2.10-1.0.jar "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW14.4 Replicate Criteo Challenge winning solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is code used to preprocess the raw dataset into one with 1000 hashed features, output in LibSVM format. If we get GBDT and feature hashing to work, this step will be unnecessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocess.py\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "from math import log, exp\n",
    "from collections import defaultdict\n",
    "import hashlib\n",
    "import sys\n",
    "\n",
    "# Calculate a feature dictionary for an observation's features based on hashing\n",
    "def hashFunction(numBuckets, rawFeats, printMapping=False):\n",
    "    mapping = {}\n",
    "    for ind, category in rawFeats:\n",
    "        featureString = category + str(ind)\n",
    "        mapping[featureString] = int(int(hashlib.md5(featureString).hexdigest(), 16) % numBuckets)\n",
    "    if(printMapping): print mapping\n",
    "    sparseFeatures = defaultdict(float)\n",
    "    for bucket in mapping.values():\n",
    "        sparseFeatures[bucket] += 1.0\n",
    "    return dict(sparseFeatures)\n",
    "\n",
    "# Converts a comma separated string into a list of (featureID, value) tuples\n",
    "def parsePoint(point):\n",
    "    feature_list = point.split(',')[1:]\n",
    "    return [(idx, feature) for idx, feature in enumerate(feature_list)]\n",
    "\n",
    "# Create a LabeledPoint for this observation using hashing\n",
    "def parseHashPoint(point, numBuckets):\n",
    "    # parse the points\n",
    "    point_list = parsePoint(point)\n",
    "    #get the label of the point\n",
    "    label = point.split(',')[0]\n",
    "    \n",
    "    features = hashFunction(numBuckets, point_list)\n",
    "    return LabeledPoint(label, SparseVector(numBuckets, features))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sc = SparkContext(appName=\"preprocess\")\n",
    "    input_file=sys.argv[1]\n",
    "    output=sys.argv[2]\n",
    "    numBucketsCTR=int(sys.argv[3])\n",
    "\n",
    "    # read in the files and parse tab to comma\n",
    "    rawData = sc.textFile(input_file, 20).map(lambda x: x.replace('\\t', ','))\n",
    "    \n",
    "    # cache the data\n",
    "    rawData.cache()\n",
    "    \n",
    "    # hash the data\n",
    "    hashData = rawData.map(lambda x: parseHashPoint(x, numBucketsCTR))\n",
    "    MLUtils.saveAsLibSVMFile(hashData, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the scala file for running FFM on the training dataset and calculating log loss and AUC on the training set. It is compiled and run the same way as the code in 14.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import com.github.fommil.netlib.BLAS._\n",
    "import com.github.fommil.netlib.BLAS.{getInstance => blas}\n",
    "\n",
    "import scala.util.Random\n",
    "\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.mllib.linalg._\n",
    "import org.apache.spark.mllib.evaluation.RegressionMetrics\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.apache.spark.mllib.regression._\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n",
    "\n",
    "\n",
    "object TestFM extends App {\n",
    "\n",
    "  /**\n",
    "    * print log loss and AUC for a model and dataset\n",
    "    * model: trained FMM model\n",
    "    * data: RDD containing training/validation/test data\n",
    "    * desc: string used as title of dataset\n",
    "    */\n",
    "  def getMetrics(model: FMModel,\n",
    "                 data: RDD[LabeledPoint],\n",
    "                 desc: String): Unit = {\n",
    "\n",
    "    val dataSize = data.count()\n",
    "    val preds = data.map { point =>\n",
    "      val prediction = model.predict(point.features)\n",
    "      (prediction, point.label)\n",
    "    }\n",
    "\n",
    "    val logLoss = preds.map { pred_label =>\n",
    "      computeLogLoss(pred_label._1, pred_label._2)\n",
    "      }.sum() / dataSize\n",
    "\n",
    "    val metrics = new BinaryClassificationMetrics(preds)\n",
    "    val auROC = metrics.areaUnderROC\n",
    "\n",
    "    println(desc + \" log loss = \" + logLoss)\n",
    "    println(\"Area under ROC = \" + auROC)\n",
    "  }\n",
    "\n",
    "  /**\n",
    "    * function to compute log loss\n",
    "    * p: model probability for given data point\n",
    "    * y: label for data point\n",
    "    */\n",
    "  def computeLogLoss(p: Double, y: Double): Double = {\n",
    "    val epsilon = 10E-12\n",
    "    var x = math.max(epsilon, p)\n",
    "    x = math.min(1 - epsilon, p)\n",
    "    val logLoss = y*math.log(x) + (1-y)*math.log(1-x)\n",
    "    return -logLoss\n",
    "  }\n",
    "\n",
    "  override def main(args: Array[String]): Unit = {\n",
    "\n",
    "    val sc = new SparkContext(new SparkConf().setAppName(\"TESTFM\"))\n",
    "\n",
    "    // process args. args 0-3 specify data locations. rest are hyperparams\n",
    "    val train_in = args(0)\n",
    "    val val_in = args(1)\n",
    "    val test_in = args(2)\n",
    "    val step_size = args(3).toDouble\n",
    "    val reg = args(4).toDouble\n",
    "    val num_iter = args(5).toInt\n",
    "\n",
    "    // read in data\n",
    "    val training = MLUtils.loadLibSVMFile(sc, train_in).cache()\n",
    "    val validate = MLUtils.loadLibSVMFile(sc, val_in).cache()\n",
    "    val test = MLUtils.loadLibSVMFile(sc, test_in).cache()\n",
    "\n",
    "    // train model\n",
    "    val fm1 = FMWithSGD.train(training, task = 1, numIterations = num_iter, stepSize = step_size, miniBatchFraction = 1.0, dim = (true, true, 4), regParam = (reg, reg, reg), initStd = 0.1)\n",
    "\n",
    "    // get metrics against training, validation, and test sets\n",
    "    getMetrics(fm1, training, \"training\")\n",
    "    getMetrics(fm1, validate, \"validation\")\n",
    "    getMetrics(fm1, test, \"test\")\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample command to run FFM on the cluster, from the project's root directory. The arguments are as follows:\n",
    "1. training set location\n",
    "2. validation set location\n",
    "3. test set location\n",
    "4. step size\n",
    "5. regulation param\n",
    "6. number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "../spark/bin/spark-submit --master spark://ec2-52-53-221-66.us-west-1.compute.amazonaws.com:7077 \\\n",
    "    target/scala-2.10/testfm_2.10-1.0.jar \\\n",
    "    s3n://<access_key>:<secret_key>@ucb-mids-mls-jamesroute-hw5/criteo-train-1000 \\\n",
    "    s3n://<access_key>:<secret_key>@ucb-mids-mls-jamesroute-hw5/criteo-valid-1000 \\\n",
    "    s3n://<access_key>:<secret_key>@ucb-mids-mls-jamesroute-hw5/criteo-test-1000 1.5 1E-6 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Results\n",
    "We run a handful of tests on the data, with a total runtime of about 60 minutes on 12 m3.xlarge instances. For now, the datasets were generated by hashing the raw data into 1000 buckets. We chose the following hyperparameter values:\n",
    "* **step size:** 0.15, 0.5, 1.25 or 1.5\n",
    "* **regulation parameter:** 0, 1E-6, 1E-3\n",
    "* **number of iterations:** 100\n",
    "\n",
    "####Table 1: Training Data LogLoss and AUC Value\n",
    "\n",
    "|Log Loss|AUC Value|Step Size|Reg Param|Number Iterations|\n",
    "|-|-|-|-|-|\n",
    "|0.5812651631286812|0.5475276055210444|0.15|0|100|\n",
    "|0.5583212569433286|0.6148138022912754|0.5|0|100|\n",
    "|**0.5318299784251057**|0.678486224625755|1.5|0|100|\n",
    "|0.581282196844917|0.5462594537466401|0.15|1e-6|100|\n",
    "|0.5562192738289558|0.6180344981091518|0.5|1e-6|100|\n",
    "|0.5339110403087934|0.6771116793841314|1.25|1e-6|1.5|\n",
    "|0.5788832000989815|0.5388756286238648|0.15|1e-3|100|\n",
    "|0.5584669577340001|0.6145967945963438|0.5|1e-3|100|\n",
    "|0.5324526740624218|**0.6789627684773109**|1.25|1e-3|100|\n",
    "\n",
    "####Table 2: Validation Data LogLoss and AUC Value\n",
    "\n",
    "|Log Loss|AUC Value|Step Size|Reg Param|Number Iterations|\n",
    "|-|-|-|-|-|\n",
    "|0.5814953783962474|0.5472909130297269|0.15|0|100|\n",
    "|0.5583624771970057|0.6150076498035781|0.5|0|100|\n",
    "|**0.531713162443586** |**0.6789444465955337**|1.5|0|100|\n",
    "|0.581460859665691|0.5460948369447726|0.15|1e-6|100|\n",
    "|0.5563839218794641|0.6180450252001811|0.5|1e-6|100|\n",
    "|0.534213952245211|0.676735543217068|1.25|1e-6|1.5|\n",
    "|0.5790336852807052|0.5387405003639277|0.15|1e-3|100|\n",
    "|0.5586476671536188|0.6146443231524853|0.5|1e-3|100|\n",
    "|0.532672536417834|0.6787517531915529|1.25|1e-3|100|\n",
    "\n",
    "####Table 3: Test Data LogLoss and AUC Value\n",
    "\n",
    "|Log Loss|AUC Value|Step Size|Reg Param|Number Iterations|\n",
    "|-|-|-|-|-|\n",
    "|0.5815648444739806|0.5476740008575337|0.15|0|100|\n",
    "|0.5584779781283143|0.6153536713548916|0.5|0|100|\n",
    "|**0.5318905726796802**|0.678818844239985|1.5|0|100|\n",
    "|0.5815725093404048|0.5463220966180165|0.15|1e-6|100|\n",
    "|0.5565032761659852|0.618282460615438|0.5|1e-6|100|\n",
    "|0.5339955470930027|0.677391148251628|1.25|1e-6|1.5|\n",
    "|0.5791791356349545|0.5389986208305242|0.15|1e-3|100|\n",
    "|0.5586332832864396|0.6149699808211845|0.5|1e-3|100|\n",
    "|0.5324809477467553|**0.6793974696153768**|1.25|1e-3|100|\n",
    "\n",
    "From the tables above, we can see that the step size of 1.5 and regulation parameter of 0 yielded the lowest log loss. A step size of 1.25 and regulation parameter of 1E-3 yielded the highest AUC.\n",
    "\n",
    "We get NaN for step size of 1.5 and non-zero regulation parameter, suggesting that the model cannot tolerate a step size this high in some circumstances. We therefore substituted a step size of 1.25 for these cases.\n",
    "\n",
    "And now for our final action in 261, we shut down the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# time to close it down\n",
    "./spark/ec2/spark-ec2 --region=us-west-1 destroy jr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
