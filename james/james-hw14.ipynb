{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW14.3 Field-aware Factorization Machine  FFM test\n",
    "**Download the Spark libFM from https://github.com/zhengruifeng/spark-libFM <br><br>\n",
    "Run the following Field-aware Factorization Machine test: <br><br>\n",
    "https://github.com/zhengruifeng/spark-libFM/blob/master/src/main/scala/TestFM.scala <br><br>\n",
    "Describe the dataset. Describe the two experiments: fm1 and fm2 and discuss your results.**\n",
    "\n",
    "**Dataset:**<br>\n",
    "The dataset comprises approximately 2.4M lines of the format: <br>\n",
    "*label feature0:value0 feature1:value1 ... featureN:valueN*\n",
    "\n",
    "The lines are in sparse format, the labels take values -1 or 1, and the feature values are either binary or take a fractional value between 0 and 1. The total uncompressed data size is about 2.2G.\n",
    "\n",
    "**Differences in the two factorizations:**\n",
    "* fm1 trains an FFM using stochastic gradient descent for 100 iterations. This is an iterative method that processes one data point at a time and adjusts the model based on the gradient of the loss function. fm1 uses all of the data in each iteration (it can be configured to use lesS).\n",
    "* fm2 trains an FFM using limited-memory Broyden–Fletcher–Goldfarb–Shanno for 20 iterations. This algorithm approximates Newton's method (using the gradient and an approximate Hessian) to find a more direct path to convergence than gradient descent.\n",
    "\n",
    "Both experiments use equivalent parameters otherwise, including for interactions (use global bias term, use one-way interactions, use 4 factors for pairwise interactions) and regularization (all regularization parameters set to 0). There is no output from FFMTest, but we can compare runtimes: with 10 m3.xlarge instances, fm1 required approximately 2 minutes and fm2 13 minutes (15 minutes total).\n",
    "\n",
    "The original scala code provides no output, so I wrote a function and created an additional RDD calculate log loss. Both fm1 and fm2 had identical log loss of 0.8520897668623415.\n",
    "\n",
    "Below are the commands and code used to set up the cluster and compile and run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up a cluster of 10 m3.xlarge instances\n",
    "./spark/ec2/spark-ec2 --key-pair=jamesr261 --identity-file=jamesr261.pem -s 12 -t m3.xlarge \\\n",
    "    --region=us-west-1 --zone=us-west-1a launch jr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the scala file for running the factorizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import com.github.fommil.netlib.BLAS._\n",
    "import com.github.fommil.netlib.BLAS.{getInstance => blas}\n",
    "\n",
    "import scala.util.Random\n",
    "\n",
    "import org.apache.spark.mllib.linalg._\n",
    "import org.apache.spark.mllib.evaluation.RegressionMetrics\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.apache.spark.mllib.regression._\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "\n",
    "object TestFM extends App {\n",
    "\n",
    "  override def main(args: Array[String]): Unit = {\n",
    "\n",
    "    val sc = new SparkContext(new SparkConf().setAppName(\"TESTFM\"))\n",
    "\n",
    "    val training = MLUtils.loadLibSVMFile(sc, \"s3n://<access_key>:<secret_key>@ucb-mids-mls-jamesroute-hw5/url_combined\", false, -1, 20).cache()\n",
    "\n",
    "    //    val task = args(1).toInt\n",
    "    //    val numIterations = args(2).toInt\n",
    "    //    val stepSize = args(3).toDouble\n",
    "    //    val miniBatchFraction = args(4).toDouble\n",
    "\n",
    "    val dataSize = training.count()\n",
    "\n",
    "    // run with SGD, report log loss\n",
    "    val fm1 = FMWithSGD.train(training, task = 1, numIterations = 100, stepSize = 0.15, miniBatchFraction = 1.0, dim = (true, true, 4), regParam = (0, 0, 0), initStd = 0.1)\n",
    "    val logLoss_fm1 = training.map { point =>\n",
    "      fm1.predict(point.features)\n",
    "    }.sum() / dataSize\n",
    "    println(\"training log loss for fm1 = \" + logLoss_fm1)\n",
    "\n",
    "    // run with LBFGS, report log loss \n",
    "    val fm2 = FMWithLBFGS.train(training, task = 1, numIterations = 20, numCorrections = 5, dim = (true, true, 4), regParam = (0, 0, 0), initStd = 0.1)\n",
    "    val logLoss_fm2 = training.map { point =>\n",
    "      fm2.predict(point.features)\n",
    "    }.sum() / dataSize\n",
    "    println(\"training log loss for fm2 = \" + logLoss_fm2)\n",
    "\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the SBT file used to build the jar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name := \"TESTFM\"\n",
    "\n",
    "version := \"1.0\"\n",
    "\n",
    "scalaVersion := \"2.10.4\"\n",
    "\n",
    "libraryDependencies ++= Seq(\n",
    "  \"org.apache.spark\" % \"spark-core_2.10\" % \"1.5.1\",\n",
    "  \"org.apache.spark\" % \"spark-mllib_2.10\" % \"1.5.1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the command to submit the job to Spark, run from the root of the project directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# driver mem set to 6G to avoid java heap memory issues\n",
    "~/spark/bin/spark-submit --driver-memory 6G \\\n",
    "    --master spark://ec2-52-53-250-84.us-west-1.compute.amazonaws.com:7077 \\\n",
    "    target/scala-2.10/testfm_2.10-1.0.jar "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW14.4 Replicate Criteo Challenge winning solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is code used to preprocess the raw dataset into one with 1000 hashed features, output in LibSVM format. If we get GBDT and feature hashing to work, this step will be unnecessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocess.py\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "from math import log, exp\n",
    "from collections import defaultdict\n",
    "import hashlib\n",
    "import sys\n",
    "\n",
    "# Calculate a feature dictionary for an observation's features based on hashing\n",
    "def hashFunction(numBuckets, rawFeats, printMapping=False):\n",
    "    mapping = {}\n",
    "    for ind, category in rawFeats:\n",
    "        featureString = category + str(ind)\n",
    "        mapping[featureString] = int(int(hashlib.md5(featureString).hexdigest(), 16) % numBuckets)\n",
    "    if(printMapping): print mapping\n",
    "    sparseFeatures = defaultdict(float)\n",
    "    for bucket in mapping.values():\n",
    "        sparseFeatures[bucket] += 1.0\n",
    "    return dict(sparseFeatures)\n",
    "\n",
    "# Converts a comma separated string into a list of (featureID, value) tuples\n",
    "def parsePoint(point):\n",
    "    feature_list = point.split(',')[1:]\n",
    "    return [(idx, feature) for idx, feature in enumerate(feature_list)]\n",
    "\n",
    "# Create a LabeledPoint for this observation using hashing\n",
    "def parseHashPoint(point, numBuckets):\n",
    "    # parse the points\n",
    "    point_list = parsePoint(point)\n",
    "    #get the label of the point\n",
    "    label = point.split(',')[0]\n",
    "    \n",
    "    features = hashFunction(numBuckets, point_list)\n",
    "    return LabeledPoint(label, SparseVector(numBuckets, features))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sc = SparkContext(appName=\"preprocess\")\n",
    "    input_file=sys.argv[1]\n",
    "    output=sys.argv[2]\n",
    "    numBucketsCTR=int(sys.argv[3])\n",
    "\n",
    "    # read in the files and parse tab to comma\n",
    "    rawData = sc.textFile(input_file, 20).map(lambda x: x.replace('\\t', ','))\n",
    "    \n",
    "    # cache the data\n",
    "    rawData.cache()\n",
    "    \n",
    "    # hash the data\n",
    "    hashData = rawData.map(lambda x: parseHashPoint(x, numBucketsCTR))\n",
    "    MLUtils.saveAsLibSVMFile(hashData, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the scala file for running FFM on the training dataset and calculating log loss and AUC on the training set. It is compiled and run the same way as the code in 14.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import com.github.fommil.netlib.BLAS._\n",
    "import com.github.fommil.netlib.BLAS.{getInstance => blas}\n",
    "\n",
    "import scala.util.Random\n",
    "\n",
    "import org.apache.spark.mllib.linalg._\n",
    "import org.apache.spark.mllib.evaluation.RegressionMetrics\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.apache.spark.mllib.regression._\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n",
    "\n",
    "\n",
    "object TestFM extends App {\n",
    "\n",
    "  def computeLogLoss(p: Double, y: Double): Double = {\n",
    "    val epsilon = 10E-12\n",
    "    if(y == 0){\n",
    "      if(p == 1){\n",
    "        return -math.log(1 - p + epsilon)\n",
    "      }else{\n",
    "        return -math.log(1 - p)\n",
    "      }\n",
    "    }else{\n",
    "      if(p == 0){\n",
    "        return -math.log(epsilon + p)\n",
    "      }else{\n",
    "        return -math.log(p)\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "\n",
    "  override def main(args: Array[String]): Unit = {\n",
    "\n",
    "    val sc = new SparkContext(new SparkConf().setAppName(\"TESTFM\"))\n",
    "\n",
    "    //    \"hdfs://ns1/whale-tmp/url_combined\"\n",
    "    val training = MLUtils.loadLibSVMFile(sc, \"s3n://<access_key>:<secret_key>@ucb-mids-mls-jamesroute-hw5/criteo-train-1000\", false, -1, 20).cache()\n",
    "\n",
    "    val dataSize = training.count()\n",
    "\n",
    "    //    val task = args(1).toInt\n",
    "    //    val numIterations = args(2).toInt\n",
    "    //    val stepSize = args(3).toDouble\n",
    "    //    val miniBatchFraction = args(4).toDouble\n",
    "\n",
    "    val fm1 = FMWithSGD.train(training, task = 1, numIterations = 100, stepSize = 0.15, miniBatchFraction = 1.0, dim = (true, true, 4), regParam = (0, 0, 0), initStd = 0.1)\n",
    "\n",
    "    val preds_fm1 = training.map { point =>\n",
    "      val prediction = fm1.predict(point.features)\n",
    "      (prediction, point.label)\n",
    "    }\n",
    "\n",
    "    val logLoss_fm1 = preds_fm1.map { pred_label =>\n",
    "      computeLogLoss(pred_label._1, pred_label._2)\n",
    "      }.sum() / dataSize\n",
    "\n",
    "    val metrics = new BinaryClassificationMetrics(preds_fm1)\n",
    "    val auROC = metrics.areaUnderROC\n",
    "\n",
    "    println(\"training log loss = \" + logLoss_fm1)\n",
    "    println(\"Area under ROC = \" + auROC)\n",
    "  }\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
