{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We followed the footsteps of the Criteo winners, per presentation below:\n",
    "http://www.csie.ntu.edu.tw/~r01922136/kaggle-2014-criteo.pdf\n",
    "\n",
    "## Start Spark Context "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# spark_home = os.environ['SPARK_HOME'] = '/usr/local/Cellar/apache-spark/1.5.0/libexec/'\n",
    "spark_home = os.environ['SPARK_HOME'] = '/usr/local/Cellar/apache-spark/1.5.0/libexec/'\n",
    "\n",
    "\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME enviroment variable is not set')\n",
    "sys.path.insert(0,os.path.join(spark_home,'python'))\n",
    "sys.path.insert(0,os.path.join(spark_home,'python/lib/py4j-0.8.2.1-src.zip'))\n",
    "execfile(os.path.join(spark_home,'python/pyspark/shell.py'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we need to parse raw data,  create an RDD of (feaureID, value) tuples for ONLY categorical values (value is not an int)\n",
    "Per PowerPoint of the Criteo Winners, we are ignoring first 14 features as 1 label and 13 integer features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parsePointForCategoricalOnly(point):\n",
    "    \"\"\"Converts a comma separated string into a list of (featureID, value) tuples ONLY IF value is not an integer. \n",
    "    Note:\n",
    "        featureIDs should start at 0 and increase to the number of features - 1.\n",
    "\n",
    "    Args:\n",
    "        point (str): A comma separated string where the first value is the label and the rest\n",
    "            are features.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of (featureID, value) tuples.\n",
    "        [(1, \"cat\"), (1,\"dog\"), ... ]\n",
    "    \"\"\"\n",
    "    values = [x for x in point.split(\"\\t\")]\n",
    "    num_features = len(values)\n",
    "    x = []\n",
    "    \n",
    "    #get only 14th feature and after\n",
    "    for i in range(14, num_features):\n",
    "            x.append((i, values[i]))\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test on sample\n",
    "rawData = sc.textFile('dac_sample.txt')\n",
    "rawData.cache()\n",
    "parsedTrainFeat = rawData.map(parsePointForCategoricalOnly)\n",
    "parsedTrainFeat.cache()\n",
    "\n",
    "print parsedTrainFeat.take(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we must create a feature of hot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "def createOneHotDict(inputData):\n",
    "    \"\"\"Creates a one-hot-encoder dictionary based on the input data.\n",
    "\n",
    "    Args:\n",
    "        inputData (RDD of lists of (int, str)): An RDD of observations where each observation is\n",
    "            made up of a list of (featureID, value) tuples.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where the keys are (featureID, value) tuples and map to values that are\n",
    "            unique integers.\n",
    "            [((3, \"cat\"),4),((1, \"dog\"),1)] \n",
    "    \"\"\"\n",
    "    return (inputData\n",
    "            .flatMap(lambda x: x)\n",
    "            .distinct()\n",
    "            .sortByKey()\n",
    "            .zipWithIndex()\n",
    "            .collectAsMap()            \n",
    "           )\n",
    "\n",
    "\n",
    "\n",
    "sampleOHEDictAuto = createOneHotDict(parsedTrainFeat)\n",
    "\n",
    "#cache it as it will be reused later\n",
    "sampleOHEDictAuto.cache()\n",
    "numCtrOHEFeats =  len(sampleOHEDictAuto)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print sampleOHEDictAuto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now forEach data we will create Labeled Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "def oneHotEncoding(rawFeats, OHEDict, numOHEFeats):\n",
    "    \"\"\"Produce a one-hot-encoding from a list of features and an OHE dictionary.\n",
    "\n",
    "    Note:\n",
    "        If a (featureID, value) tuple doesn't have a corresponding key in OHEDict it should be\n",
    "        ignored.\n",
    "\n",
    "    Args:\n",
    "        rawFeats (list of (int, str)): The features corresponding to a single observation.  Each\n",
    "            feature consists of a tuple of featureID and the feature's value. (e.g. sampleOne)\n",
    "        OHEDict (dict): A mapping of (featureID, value) to unique integer.\n",
    "        numOHEFeats (int): The total number of unique OHE features (combinations of featureID and\n",
    "            value).\n",
    "\n",
    "    Returns:\n",
    "        SparseVector: A SparseVector of length numOHEFeats with indicies equal to the unique\n",
    "            identifiers for the (featureID, value) combinations that occur in the observation and\n",
    "            with values equal to 1.0.\n",
    "    \"\"\"\n",
    "    featureArray = []\n",
    "    for featTuple in rawFeats:\n",
    "        \n",
    "        if featTuple in OHEDict: #tuple is in the dict \n",
    "            featureArray.append(OHEDict[featTuple])\n",
    "        else: #skip it\n",
    "            pass\n",
    "    \n",
    "    #sort the indices\n",
    "    featureArray.sort()\n",
    "    \n",
    "    #we are assuming no duplicates, thus occurrence of each feature is 1 (last parameter in SparseVector)\n",
    "    return SparseVector(numOHEFeats, featureArray,[1] * len(featureArray))\n",
    "\n",
    "\n",
    "def parseOHEPoint(point, OHEDict, numOHEFeats, getRaw=False):\n",
    "    \"\"\"Obtain the label and feature vector for this raw observation.\n",
    "\n",
    "    Note:\n",
    "        You must use the function `oneHotEncoding` in this implementation or later portions\n",
    "        of this lab may not function as expected.\n",
    "\n",
    "    Args:\n",
    "        point (str): A comma separated string where the first value is the label and the rest\n",
    "            are features.\n",
    "        OHEDict (dict of (int, str) to int): Mapping of (featureID, value) to unique integer.\n",
    "        numOHEFeats (int): The number of unique features in the training dataset.\n",
    "        getRaw: if false, return LabelPoint with SparseVector for categorical features. \n",
    "                if true, return \"\\t\" delimited mixture of integer and categorical features \n",
    "                        with categorical features having value of either 0 or 1 \n",
    "\n",
    "    Returns:\n",
    "        LabeledPoint: Contains the label for the observation and the one-hot-encoding of the\n",
    "            raw features based on the provided OHE dictionary.\n",
    "    \"\"\"\n",
    "    values = [x for x in point.split(\"\\t\")]\n",
    "    num_features = len(values)\n",
    "    x = []\n",
    "    \n",
    "    #per Criteo Winner's Powerpoint we will treat only 14th feature and later as categorical\n",
    "    for i in range(14, num_features):\n",
    "        x.append((i, values[i]))\n",
    "    features = oneHotEncoding(x, OHEDict, numOHEFeats)\n",
    "    \n",
    "    if getRaw: #must collect label and integer features in addition to the categorical ones\n",
    "        rawDataArray = []\n",
    "        \n",
    "        #get label and integer features\n",
    "        for i in range(0,13):\n",
    "            rawDataArray.append(values[i])\n",
    "        \n",
    "        #now convert SparseVector features to \n",
    "        rawDataArray.extend([str(int(x)) for x in features.toArray().tolist()])\n",
    "        return \"\\t\".join(rawDataArray)\n",
    "        \n",
    "    else: \n",
    "        return LabeledPoint(values[0], features)\n",
    "\n",
    "\n",
    "#OHETrainData = rawData.map(lambda point: parseOHEPoint(point, sampleOHEDictAuto, numCtrOHEFeats))\n",
    "#OHETrainData.cache()\n",
    "#print OHETrainData.take(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bleh = SparseVector(20, [3,4],[1,1])\n",
    "print bleh.toArray().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then we will count each features so we can prune features that don't happen too often "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#def bucketFeatByCount(featCount):\n",
    "#    \"\"\"Bucket the counts by powers of two.\"\"\"\n",
    "#    for i in range(11):\n",
    "#        size = 2 ** i\n",
    "#        if featCount <= size:\n",
    "#            return size\n",
    "#    return -1\n",
    "\n",
    "featCounts = (OHETrainData\n",
    "              .flatMap(lambda lp: lp.features.indices)\n",
    "              .map(lambda x: (x, 1))\n",
    "              .reduceByKey(lambda x, y: x + y)).sortByKey()\n",
    "\n",
    "print featCounts.collect()\n",
    "#featCountsBuckets = (featCounts\n",
    "#                     .map(lambda x: (bucketFeatByCount(x[1]), 1))\n",
    "#                     .filter(lambda (k, v): k != -1)\n",
    "#                     .reduceByKey(lambda x, y: x + y)\n",
    "#                     .collect())\n",
    "#print featCountsBuckets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now we will prune the original dictionary \n",
    ".sortByKey() #.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#limit = 10\n",
    "\n",
    "#prunedOHCDictionary = sc.parallelize(sampleOHEDictAuto.items()).map(lambda x: (x[1],x[0])).join(featCounts).filter(lambda keyValue: keyValue[1][1] >= limit).sortByKey().map(lambda x:(x[1][0],x[0]))  #.collectAsMap() #take(10)\n",
    "\n",
    "#print prunedOHCDictionary.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "limit = 10\n",
    "\n",
    "prunedOHCDictionary = sc.parallelize(sampleOHEDictAuto.items()).map(lambda x: (x[1],x[0])).join(featCounts).filter(lambda keyValue: keyValue[1][1] >= limit).sortByKey().map(lambda x: x[1][0]).zipWithIndex().collectAsMap() #take(10)     \n",
    "\n",
    "print prunedOHCDictionary.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we will re-encode the original data using new dictionary\n",
    "but we never played with the mixture of integer and \n",
    "\n",
    "maybe for each feature it wold be 1 and 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OHETrainDataPruned = rawData.map(lambda point: parseOHEPoint(point, prunedOHCDictionary, len(prunedOHCDictionary),True))\n",
    "OHETrainDataPruned.cache()\n",
    "print OHETrainDataPruned.take(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile CriteoFirstPreprocessOHE.py\n",
    "import sys\n",
    "import ast\n",
    "import json\n",
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "\n",
    "\n",
    "#Compile a tuple of categorical feature and its value\n",
    "def parsePointForCategoricalOnly(point):\n",
    "    \"\"\"Converts a comma separated string into a list of (featureID, value) tuples ONLY IF value is not an integer. \n",
    "    Note:\n",
    "        featureIDs should start at 0 and increase to the number of features - 1.\n",
    "\n",
    "    Args:\n",
    "        point (str): A comma separated string where the first value is the label and the rest\n",
    "            are features.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of (featureID, value) tuples.\n",
    "        [(1, \"cat\"), (1,\"dog\"), ... ]\n",
    "    \"\"\"\n",
    "    values = [x for x in point.split(\"\\t\")]\n",
    "    num_features = len(values)\n",
    "    x = []\n",
    "    \n",
    "    #get only 14th feature and after\n",
    "    for i in range(14, num_features):\n",
    "            x.append((i, values[i]))\n",
    "    return x\n",
    "\n",
    "#create one-hot-encoder dictionary given a raw input\n",
    "def createOneHotDict(rawData):\n",
    "    \"\"\"Creates a one-hot-encoder dictionary based on the input data.\n",
    "\n",
    "    Args:\n",
    "        inputData (RDD of lists of (int, str)): An RDD of observations where each observation is\n",
    "            made up of a list of (featureID, value) tuples.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where the keys are (featureID, value) tuples and map to values that are\n",
    "            unique integers.\n",
    "            [((3, \"cat\"),4),((1, \"dog\"),1)] \n",
    "    \"\"\"\n",
    "    #get tuples for each \n",
    "    #inputData = rawData.map(parsePointForCategoricalOnly)\n",
    "    #print parsedTrainFeat.take(1)\n",
    "    \n",
    " \n",
    "    return (rawData\n",
    "            .map(parsePointForCategoricalOnly) #get tuple of all of the features\n",
    "            .flatMap(lambda x: x) #everything here and below is to create a dictionary\n",
    "            .distinct()\n",
    "            .sortByKey()\n",
    "            .zipWithIndex()\n",
    "            .collectAsMap()            \n",
    "           )\n",
    "\n",
    "def oneHotEncoding(rawFeats, OHEDict, numOHEFeats):\n",
    "    \"\"\"Produce a one-hot-encoding from a list of features and an OHE dictionary.\n",
    "\n",
    "    Note:\n",
    "        If a (featureID, value) tuple doesn't have a corresponding key in OHEDict it should be\n",
    "        ignored.\n",
    "\n",
    "    Args:\n",
    "        rawFeats (list of (int, str)): The features corresponding to a single observation.  Each\n",
    "            feature consists of a tuple of featureID and the feature's value. (e.g. sampleOne)\n",
    "        OHEDict (dict): A mapping of (featureID, value) to unique integer.\n",
    "        numOHEFeats (int): The total number of unique OHE features (combinations of featureID and\n",
    "            value).\n",
    "\n",
    "    Returns:\n",
    "        SparseVector: A SparseVector of length numOHEFeats with indicies equal to the unique\n",
    "            identifiers for the (featureID, value) combinations that occur in the observation and\n",
    "            with values equal to 1.0.\n",
    "    \"\"\"\n",
    "    featureArray = []\n",
    "    for featTuple in rawFeats:\n",
    "        \n",
    "        if featTuple in OHEDict: #tuple is in the dict \n",
    "            featureArray.append(OHEDict[featTuple])\n",
    "        else: #skip it\n",
    "            pass\n",
    "    \n",
    "    #sort the indices\n",
    "    featureArray.sort()\n",
    "    \n",
    "    #we are assuming no duplicates, thus occurrence of each feature is 1 (last parameter in SparseVector)\n",
    "    return SparseVector(numOHEFeats, featureArray,[1] * len(featureArray))\n",
    "\n",
    "#for each line of data return either sparse vector or raw data, given a dictionary\n",
    "def parseOHEPoint(point, OHEDict, getRaw=False):\n",
    "    \"\"\"Obtain the label and feature vector for this raw observation.\n",
    "\n",
    "    Note:\n",
    "        You must use the function `oneHotEncoding` in this implementation or later portions\n",
    "        of this lab may not function as expected.\n",
    "\n",
    "    Args:\n",
    "        point (str): A comma separated string where the first value is the label and the rest\n",
    "            are features.\n",
    "        OHEDict (dict of (int, str) to int): Mapping of (featureID, value) to unique integer.\n",
    "        getRaw: if false, return LabelPoint with SparseVector for categorical features. \n",
    "                if true, return \"\\t\" delimited mixture of integer and categorical features \n",
    "                        with categorical features having value of either 0 or 1 \n",
    "\n",
    "    Returns:\n",
    "        LabeledPoint: Contains the label for the observation and the one-hot-encoding of the\n",
    "            raw features based on the provided OHE dictionary.\n",
    "    \"\"\"\n",
    "    numOHEFeats = len(OHEDict)\n",
    "    #print >> sys.stderr, OHEDict\n",
    "\n",
    "    \n",
    "    values = [x for x in point.split(\"\\t\")]\n",
    "    num_features = len(values)\n",
    "    x = []\n",
    "    \n",
    "    #per Criteo Winner's Powerpoint we will treat only 14th feature and later as categorical\n",
    "    for i in range(14, num_features):\n",
    "        x.append((i, values[i]))\n",
    "    features = oneHotEncoding(x, OHEDict, numOHEFeats)\n",
    "    \n",
    "    if getRaw: #must collect label and integer features in addition to the categorical ones\n",
    "        rawDataArray = []\n",
    "        \n",
    "        #get label and integer features\n",
    "        for i in range(0,14):\n",
    "            rawDataArray.append(values[i])\n",
    "        \n",
    "        #now convert SparseVector features to \n",
    "        rawDataArray.extend([str(int(x)) for x in features.toArray().tolist()])\n",
    "        return \"\\t\".join(rawDataArray)\n",
    "        \n",
    "    else: \n",
    "        return LabeledPoint(values[0], features)\n",
    "\n",
    "def create_LIBSVM(point):\n",
    "    values = [x for x in point.split(\"\\t\")]\n",
    "    num_features = len(values)\n",
    "    output = [values[0]]\n",
    "    for i in range(1, num_features):\n",
    "        input = str(i) + \":\" + str(values[i])\n",
    "        output.append(input)\n",
    "    return \"\\t\".join(output)\n",
    "\n",
    "def main_preprocessWithOHE(input_file, limit, output_file):\n",
    "    \n",
    "    #test on sample\n",
    "    rawData = sc.textFile(input_file) #'dac_sample.txt'\n",
    "    rawData.cache() #cache since it will be used later to generate final preprocessed output\n",
    "    \n",
    "    prelimOHEDict = createOneHotDict(rawData) \n",
    "    \n",
    "    #apply OHE to each data based on dictionary created above\n",
    "    prelimOHETrainData = rawData.map(lambda point: parseOHEPoint(point, prelimOHEDict))\n",
    "    \n",
    "    #now get count of each feature so we can prune them\n",
    "    featCounts = (prelimOHETrainData\n",
    "              .flatMap(lambda lp: lp.features.indices)\n",
    "              .map(lambda x: (x, 1))\n",
    "              .reduceByKey(lambda x, y: x + y)).sortByKey()\n",
    "    \n",
    "    #based on the count prune and create new dictionary - take top 26 features with highest counts\n",
    "    prunedOHEDictionary = sc.parallelize(prelimOHEDict.items()) \\\n",
    "              .map(lambda x: (x[1],x[0])) \\\n",
    "              .join(featCounts) \\\n",
    "              .map(lambda x: (int(x[1][1]), x[1][0]))  \\\n",
    "              .sortByKey(False) \\\n",
    "              .take(26)\n",
    "    prunedOHEDictionary = sc.parallelize(prunedOHEDictionary)  \\\n",
    "                           .map(lambda x: x[1]).zipWithIndex().collectAsMap()\n",
    "    \n",
    "    #parse raw data again and create features\n",
    "    finalTrainData = rawData.map(lambda point: parseOHEPoint(point, prunedOHEDictionary, True))\n",
    "    \n",
    "    libSVMTrainData = finalTrainData.map(lambda point: create_LIBSVM(point))\n",
    "    libSVMTrainData.saveAsTextFile(output_file)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # three arguments\n",
    "    #  1. input file\n",
    "    #  2. limit\n",
    "    #  3. output file\n",
    "    \n",
    "    if len(sys.argv) < 3:\n",
    "        print >> sys.stderr, \"Usage: input_file limit_for_frequency_of_features <output_file>\"\n",
    "        exit(-1)\n",
    "\n",
    "    [input_file, limit, output_file] = sys.argv[1:4]\n",
    "    \n",
    "    sc = SparkContext(appName=\"PreprocessOHE\")\n",
    "    main_preprocessWithOHE(input_file, limit, output_file)    \n",
    "    sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#run locally\n",
    "\n",
    "#ensure folder with output name does not exist\n",
    "!rm -r output\n",
    "\n",
    "#submit spark job (note that page_rank.py has 4 arguments)\n",
    "!time /usr/local/Cellar/apache-spark/1.5.0/libexec/bin/spark-submit CriteoFirstPreprocessOHE.py dac_sample.txt 10 output     \n",
    "\n",
    "#output results:\n",
    "!rm output/_SUCCESS\n",
    "!echo '-----------------OUTPUT-----------------'\n",
    "!cat output/*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!sed 1d output/* > output/output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
